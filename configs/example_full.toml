# This is an example showing the full list of settings. For smaller examples, see
# the other files in the same folder. The defaults are listed for all options.

# interactive mode allows you to edit tomls, retry captions, etc
#
# recommendation: do an captioning round without interactive, then enable it to have
#                 more control and improve the final captions
interactive = true

# using multiple rounds allow you to run the captioning model multiple times on the
# same user prompt then have a final captioning round where the model can use the
# previous captions to generate an improved final caption
#
# from experience, this depends a lot on how good the model is as captioning errors
# can accumulate and lead to a worse caption
#
# recommendation: in most scenarios, setting this to 1 works well enough
rounds = 1

# if you want to use a different file suffix for the caption, you can change this
caption_suffix = '.txt'

# if enabled, the captioner will override the existing captions. otherwise, the image will
# be skipped
#
# recommendation: use in conjunction with interactive mode
overwrite_captions = false

# which user environment to use
#
# recommendation: set this if you want to set up the api section outside of the TOML file
env = "default"


[api]
# only the hostname and port should be set here, without any other url paths
url = "http://localhost:5001/v1"

# fill this in with your api token (necessary for public endpoints such as OpenAI's)
# if you are hosting the api locally, you leave this empty
token = ""

# the model which should be used (e.g. gpt-5-mini)
# for more details, see README.md
model_name = ""



[reasoning]
# enables chain-of-though / extra reasoning behaviour for models that support it
# using this option should produce better outputs, at the cost of inference cost and/or time
#
# recommendation: set to false unless the captioning is not good enough
enable = false

# the amount of thinking a model should do
# some model providers/apis offer more control over how much thinking budget the model has
# for simplification, this is kept as just 3 options and each captioner can decide what
# the budget will actually be
#
# options: low, medium, high
# recommendation: start with low, then go higher if needed
thinking_effort = "low"

# excludes the thought/reasoning section of the output
# since providers might hide this from the actual response, you might not see the reasoning regardless
# of the setting here
#
# recommendation: set as true and disable it if you want to see more details about what the model is thinking
exclude_from_output = true

[reasoning.advanced]
# allows you to control which thinking tokens should be used
#
# recommendation: leave it as the default, unless the model uses a different token (e.g. Kimi-VL)
thinking_start = "<think>"
thinking_end = "</think>"



[prompt]
# allows you to set the which prompt template to use
# the captioner will look in the current working directory (the directory from where
# the cli is called), then in the predefined templates in yadc/templates
# an error is given if the prompt template cannot be found
#
# recommendation: unless you want to share prompt templates between different dataset
#                 configs, you should set "template" instead of "name"
# recommendation: to avoid confusion, using absolute paths instead of relative is recommended
name = "default"

# allows you to override the prompt template used in the captioning process
# if not set, the template used will be either the one set in name
# or the default template
#
# for more details, see README.md
template = ""



[settings]
# controls how many tokens the model is allowed to output
# recommendation: 512 tokens is enough for most captioning scenarios
#                 1024 for when reasoning is enabled with low thinking effort
#                 1536 for when reasoning is enabled with medium thinking effort
#                 1536-2048 for when reasoning is enabled with high thinking effort
max_tokens = 512

# official OpenAI endpoints can store conversations for you for viewing later
# when this is set to false, this will tell the api to not store the conversation
#
# recommendation: leave it as false (especially for large datasets) when using official apis,
#                 unless you want to review the conversation later
store_conversation = false

# official OpenAI endpoints (and possible other local apis) allow you to set the quality of
# the image when embedding it into tokens for the model to use
#
# options: auto, high, low
# recommendation: auto
image_quality = "auto"


[settings.advanced]
# in most scenarios, you don't need to change this from the default. this options is for the official
# OpenAI, since o4 and newer models use 'developer' instead
#
# options: system, developer
# recommendation: leave it as system unless you use newer models from the OpenAI API directly
system_role = "system"

# added here for completion's sake, it does not allow you to change it from the default at the moment
#
# options: user
user_role = "user"

# prefill the AI's response
# some APIs do not support this, so make sure they do before using it
#
# recommendation: use a thinking token (e.g. <think>) if the model requires it
assistant_prefill = ""

# settings.advanced section will allow you to add more fields here than defined above
# change them as needed and based on the API you are actually using
#
# note: setting to null will remove these settings from the API requests
# note: some options are provided in the settings section since they can
#       be used across different APIs, so try not to override those ones
#       unless you need to
#
# some examples below (from OpenAI documentation):
temperature = 1.0
frequency_penalty = 0.0


[dataset]
# a list of paths where the captioner will search for images
# the captioner will only look at the images within the paths given below, so if you have subfolders
# that you also want to caption within any of the paths, you also need to add them to the list
paths = [
    "path_to_your_dataset_1",
    "path_to_your_dataset_2",
]

# if you wish to override certain toml properties for each image, you can set them below
# be careful when using this, as these properties will be merged with any existing ones
# from the toml associated with the image
# 
# recommendation: create a toml file for each image instead of setting them here, or
#                 use interactive mode in order to create toml files during captioning
[[dataset.images]]
# the path is necessary to know which image the properties are assigned to
path = "path_to_your_image_1"
# whatever additional properties you might want to use in your prompt templates
foo = "bar"
baz = "quiz"

[[dataset.images]]
path = "path_to_your_image_2"
foo = "bar2"

